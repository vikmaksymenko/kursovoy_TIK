<!DOCTYPE html>
<html><head>
<meta charset="utf-8">
<title>Код Хаффмана</title>
<div><p><b>Алгоритм Хаффмана</b>&nbsp;— <a href="http://ru.wikipedia.org/wiki/%D0%90%D0%B4%D0%B0%D0%BF%D1%82%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC" title="Адаптивный алгоритм">адаптивный</a> <a href="http://ru.wikipedia.org/w/index.php?title=%D0%9F%D1%80%D0%BE%D1%81%D1%82%D0%BE%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC&amp;action=edit&amp;redlink=1" class="new" title="Простой алгоритм (страница отсутствует)">простой алгоритм</a> оптимального <a href="http://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B5%D1%84%D0%B8%D0%BA%D1%81%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BE%D0%B4" title="Префиксный код">префиксного</a> <a href="http://ru.wikipedia.org/wiki/%D0%AD%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D0%B9%D0%BD%D0%BE%D0%B5_%D0%BA%D0%BE%D0%B4%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5" title="Энтропийное кодирование">кодирования</a> алфавита с минимальной <a href="http://ru.wikipedia.org/wiki/%D0%98%D0%B7%D0%B1%D1%8B%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C" title="Избыточность" class="mw-redirect">избыточностью</a>. Был разработан в <a href="http://ru.wikipedia.org/wiki/1952_%D0%B3%D0%BE%D0%B4" title="1952 год">1952&nbsp;году</a> аспирантом <a href="http://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%81%D1%81%D0%B0%D1%87%D1%83%D1%81%D0%B5%D1%82%D1%81%D0%BA%D0%B8%D0%B9_%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B8%D0%BD%D1%81%D1%82%D0%B8%D1%82%D1%83%D1%82" title="Массачусетский технологический институт">Массачусетского технологического института</a> <a href="http://ru.wikipedia.org/wiki/%D0%A5%D0%B0%D1%84%D1%84%D0%BC%D0%B0%D0%BD,_%D0%94%D1%8D%D0%B2%D0%B8%D0%B4" title="Хаффман, Дэвид">Дэвидом Хаффманом</a> при написании им курсовой работы. В настоящее время используется во многих программах сжатия данных.</p>
<p>В отличие от <a href="http://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%A8%D0%B5%D0%BD%D0%BD%D0%BE%D0%BD%D0%B0_%E2%80%94_%D0%A4%D0%B0%D0%BD%D0%BE" title="Алгоритм Шеннона — Фано">алгоритма Шеннона&nbsp;— Фано</a>, алгоритм Хаффмана остаётся всегда оптимальным и для <a href="http://ru.wikipedia.org/wiki/%D0%92%D1%82%D0%BE%D1%80%D0%B8%D1%87%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BB%D1%84%D0%B0%D0%B2%D0%B8%D1%82" title="Вторичный алфавит" class="mw-redirect">вторичных алфавитов</a> m<sub>2</sub> с более чем двумя символами.</p>
<p>Этот метод кодирования состоит из двух основных этапов:</p>
<ol>
<li>Построение оптимального кодового дерева.</li>
<li>Построение отображения код-символ на основе построенного дерева.</li>
</ol>


<p></p>
<h2>Кодирование Хаффмана</h2>
<p>Один из первых алгоритмов эффективного кодирования информации был 
предложен Д.&nbsp;А.&nbsp;Хаффманом в 1952 году. Идея алгоритма состоит в
 следующем: зная вероятности символов в сообщении, можно описать 
процедуру построения кодов переменной длины, состоящих из целого 
количества битов. Символам с большей вероятностью ставятся в 
соответствие более короткие коды. Коды Хаффмана обладают свойством <a href="http://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B5%D1%84%D0%B8%D0%BA%D1%81%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BE%D0%B4" title="Префиксный код">префиксности</a> (т.е. ни одно кодовое слово не является префиксом другого), что позволяет однозначно их декодировать.</p>
<p>Классический алгоритм Хаффмана на входе получает таблицу частот 
встречаемости символов в сообщении. Далее на основании этой таблицы 
строится дерево кодирования Хаффмана (Н-дерево). <sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup></p>
<ol>
<li>Символы входного алфавита образуют список свободных узлов. Каждый 
лист имеет вес, который может быть равен либо вероятности, либо 
количеству вхождений символа в сжимаемое сообщение.</li>
<li>Выбираются два свободных узла дерева с наименьшими весами.</li>
<li>Создается их родитель с весом, равным их суммарному весу.</li>
<li>Родитель добавляется в список свободных узлов, а два его потомка удаляются из этого списка.</li>
<li>Одной дуге, выходящей из родителя, ставится в соответствие бит 1, другой&nbsp;— бит 0.</li>
<li>Шаги, начиная со второго, повторяются до тех пор, пока в списке 
свободных узлов не останется только один свободный узел. Он и будет 
считаться корнем дерева.</li>
</ol>
<p>Допустим, у нас есть следующая таблица частот:</p>
<table class="wikitable">
<tbody><tr>
<th>15</th>
<th>7</th>
<th>6</th>
<th>6</th>
<th>5</th>
</tr>
<tr>
<td>А</td>
<td>Б</td>
<td>В</td>
<td>Г</td>
<td>Д</td>
</tr>
</tbody></table>
<p>Этот процесс можно представить как построение <a href="http://ru.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%28%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B3%D1%80%D0%B0%D1%84%D0%BE%D0%B2%29" title="Дерево (теория графов)">дерева</a>,
 корень которого&nbsp;— символ с суммой вероятностей объединенных 
символов, получившийся при объединении символов из последнего шага, его n<sub>0</sub> потомков&nbsp;— символы из предыдущего шага и&nbsp;т.&nbsp;д.</p>
<p>Чтобы определить код для каждого из символов, входящих в сообщение, 
мы должны пройти путь от корня до листа дерева, соответствующего 
текущему символу, накапливая биты при перемещении по ветвям дерева 
(первая ветвь в пути соответствует младшему биту). Полученная таким 
образом последовательность битов является кодом данного символа, 
записанным в обратном порядке.</p>
<p>Для данной таблицы символов коды Хаффмана будут выглядеть следующим образом.</p>
<table class="wikitable">
<tbody><tr>
<th>А</th>
<th>Б</th>
<th>В</th>
<th>Г</th>
<th>Д</th>
</tr>
<tr>
<td>0</td>
<td>100</td>
<td>101</td>
<td>110</td>
<td>111</td>
</tr>
</tbody></table>
<p>Поскольку ни один из полученных кодов не является префиксом другого, 
они могут быть однозначно декодированы при чтении их из потока. Кроме 
того, наиболее частый символ сообщения А закодирован наименьшим 
количеством бит, а наиболее редкий символ Д&nbsp;— наибольшим.</p>
<p>При этом общая длина сообщения, состоящего из приведённых в таблице 
символов, составит 87 бит (в среднем 2,2308 бита на символ). При 
использовании равномерного кодирования общая длина сообщения составила 
бы 117 бит (ровно 3 бита на символ). Заметим, что <a href="http://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F" title="Информационная энтропия">энтропия</a> источника, независимым образом порождающего символы с указанными частотами, составляет ~2,1858 бита на символ, т.е. <a href="http://ru.wikipedia.org/wiki/%D0%98%D0%B7%D0%B1%D1%8B%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8" title="Избыточность информации">избыточность</a>
 построенного для такого источника кода Хаффмана, понимаемая, как 
отличие среднего числа бит на символ от энтропии, составляет менее 0,05 
бит на символ.</p>
<p>Классический алгоритм Хаффмана имеет ряд существенных недостатков. 
Во-первых, для восстановления содержимого сжатого сообщения декодер 
должен знать таблицу частот, которой пользовался кодер. Следовательно, 
длина сжатого сообщения увеличивается на длину таблицы частот, которая 
должна посылаться впереди данных, что может свести на нет все усилия по 
сжатию сообщения. Кроме того, необходимость наличия полной частотной 
статистики перед началом собственно кодирования требует двух проходов по
 сообщению: одного для построения модели сообщения (таблицы частот и 
Н-дерева), другого для собственно кодирования. Во-вторых, избыточность 
кодирования обращается в ноль лишь в тех случаях, когда вероятности 
кодируемых символов являются обратными степенями числа 2. В-третьих, для
 источника с энтропией, не превышающей 1 (например, для двоичного 
источника), непосредственное применение кода Хаффмана бессмысленно.</p>
</body></html>